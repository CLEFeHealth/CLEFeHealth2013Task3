Draft README for Patient Queries & Relevance Set [generated 26/03/2013]
=======================================================================

The query and result set will be distributed as part of the CLEFeHealth 2013 Task 3: "Information retrieval to address questions patients may have when reading clinical reports".

It is now intended that the official training query and result set for Task 3 will be distributed in the coming fortnight. This set will consist of 5 queries and corresponding result set generated from manual relevance assessment (by medical professsionals) on a shallow pool.

This draft README for the query and relevance set provides specific details on the training queries and result set. It also provides a sample pseudo-query.


*****************************************************
Query and Result Set Details Summary:
-------------------------------------
-Training Set contains [released before 15 April]:
--5 queries and associated result set.

-Query Test Set contains [released 24 April]:
--50 queries.

-Result Set for Test Queries [released 1 June]:
--generated by pooling runs submitted by task participants on 1st May, and having nursing professionals conduct manual relevance assessment on the pooled sets.


*****************************************************

Query Set Details:
------------------
The goal of CLEFeHealth Task 3 is to answer patient queries about their disorders, once they have examined their discharge summary.

The discharge summaries used for this Task 3, were taken from the set used in CLEFeHealth Tasks 1 and 2. [In Tasks 1 and 2: discharge summaries which were extracted from the de-identified clinical free-text notes set of the MIMIC II database{Version 2.5, \url{http://mimic.physionet.org}. Disorders were identified within discharge summaries and linked to the matching UMLS (Unified Medical Language System) concept.]

The queries generated for Task 3 are intended to be representative of real patients information needs and statements on reading the discharge summary for the first time. It is observed in analysis of patient query logs to web based medical information services that patients typically enter very short queries (average length less than two words). However, different patients will have different information needs associated with the same query statement. For example, a patient that receives a cancer diagnosis for the first time would have a different information need than a patient at a terminal cancer stage; and patients will have different information needs depending on their age, gender, professional level, etc. This type of contextual information related to the patient history is contained in, or can be inferred from, the discharge summary in many instances. Thus, we use the discharge summaries for contextually focused generation of queries; and in places where details for contextualising are missing, we assign pseudo contextualisation. The information in discharge summary can then be used to determine the relevance of retrieved information to the specific user.

A query will be generated for a given disorder and a discharge summary. To make the query generation process more structured, we have grouped possible patients’ information needs into three main
scenarios:
--the patient has a short-term disease, or has been hospitalised after an accident (little to no knowledge of the disorder, short term treatment)
--the patient has a chronic disease or a long term disease that has just been diagnosed (little to no knowledge of the disorder, long term treatment)
--the patient has a chronic or long term disease, and this is the n-th diagnosis (potentially good knowledge of the disorder, long term treatment)

Queries to be used in this Task 3 are being created by nurses involved in the ShARe/CLEF eHealth consortium. This solution was chosen in place of recruiting patients because of the issues involved with recruitment and privacy. We believe that, being on a daily basis in contact with patients receiving treatments and discharge summaries, nurses are familiar with patients information needs and patient profiles.

65 disorders have been randomly selected from the set of 1,006 disorders individuated in the ShARe/CLEF eHealth Tasks 1. For each disorder, a discharge summary containing the disorder itself was randomly selected. Using the pairs of disorder and associated discharge summary, the nurses are developing a set of patient queries (and criteria for judging the relevance of documents to the queries, for use in the relevance assessment task).

Queries will be distributed for use in Task 3 in a classical TREC style format:

title: Text of the query
description: Longer description of what the query means
narrative: What is expected to be found in the documents relevant to that query

The following example outlines what a query will look like in the collection:

<query>
  <title>thrombocytopenia treatment corticosteroids length
  </title>
  <desc>How long should the corticosteroids treatment to cure thrombocytopenia be?
  </desc>
  <narr>Documents should contain information about treatments of thrombocytopenia, and especially corticosteroids. It should describe the treatment, its duration and how the disease is cured using it.
  <scenario>The patient has a short-term disease, or has been hospitalised after an accident (little to no knowledge of the disorder, short term treatment)</scenario>
  <profile>Professional female</profile>
  </narr>
</query>

We aim to produce at least 50 queries acceptable for use in the task using this approach. 65 disorders were selected (i.e. more than the targeted number of queries) because some disorders/queries may not be answerable using web pages from the document collection.
During the query generation process, nurses are manually removing disorders from the list of 65 that did not allow for realistic query generation. 

For each query, an information retrieval system that implements a standard BM25 weighting schema is being used to retrieve a shallow pool of documents. This is being used to assess whether a standard retrieval system could match at least one relevant document to a candidate query. Queries with no relevant documents retrieved in the shallow pool are being removed from the query set.


*****************************************************
Result Set Details:
-------------------
Result sets for the 5 training queries are also being distributed as part of the training set. Relevance assessments for these queries are being formed on pooled sets generated using the Vector Space Model and BM25. Pooled sets were generated by taking the top 30 ranked documents returned by the two retrieval models using a round robin approach with duplicates removed. 

Documents in the pooled result sets are being rated as relevant or not relevant to the queries by one nursing professional involved in the CLEF task, using the instructions for judging relevance given in each query topic.

Result sets will be generated for the 50 test queries using a post submission relevance assessment approach. Specifically, relevance assessments will be conducted on the test queries after task participants have submitted their runs. Task participants will be asked to submit up to seven runs; for each participating team, each participant will be required to submit is a baseline run that does not incorporate any sophisticated annotation, query expansion, etc. techniques.
Participants will be allowed to submit up to three runs generated using the discharge summaries associated with the queries, and up to three runs that do not use the discharge summaries. Methods to generate pooled result sets will be considered, depending on the number of submitted runs. Methods include, for example, pooling the baseline and top ranked runs submitted by task participants using a round robin approach. Generated pooled result sets will be assessed by the nurses on a binary scale (0 = irrelevant, 1 = relevant). In this process the standard TREC\_EVAL\footnote{\url{http://trec.nist.gov/trec\_eval/} (March, 2013)} will be used to determine the effectiveness of submitted runs. We anticipate using mean average precision (MAP) and precision at 10 (P@10) as evaluation measures for this task.


*****************************************************

Acknowledgements:
-----------------

This work is supported by funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement no257528 (KHRESMOI).


Disclaimer:
-----------
This query set is provided as is. No responsibility for its content are taken by either the collection creators, the Khresmoi project, nor the CLEFeHealth 2013 task organisers. 


*****************************************************
