<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2013 TASK 3 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2013 Task 3 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2013 TASK 3 Results - MEDINFO</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2013 TASK 3 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the highest priority run that used the discharge summaries (run 2) and the highest priority run that did not used the discharge summaries (run 5). As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtestXX id); others did not retrieve results for some queries. To fairly evaluate the runs of every participant, submitted runs containing formatting errors have been corrected; an artificial document (fakeCLEF) has been added to the result rankings for queries where no result was retrieved. Your runs as have been evaluated are contained in the folder <i>runs</i>.</br>Please refer to the <a href="https://sites.google.com/site/shareclefehealth/">ShARe/CLEF 2013 eHealth Evaluation Lab website</a> for additional details on the task.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2013ehealth.1-50-test.bin.final.20062013.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2013ehealth.1-50-test.bin.final.20062013.txt runName</pre>
 <h4>MEDINFO.1.3.noadd</h4>
            <pre>
runid                 	all	MEDINFO13
num_q                 	all	50
num_ret               	all	47333
num_rel               	all	1878
num_rel_ret           	all	1663
map                   	all	0.3131
gm_map                	all	0.1205
Rprec                 	all	0.3154
bpref                 	all	0.3779
recip_rank            	all	0.5892
iprec_at_recall_0.00  	all	0.6519
iprec_at_recall_0.10  	all	0.5805
iprec_at_recall_0.20  	all	0.5267
iprec_at_recall_0.30  	all	0.4631
iprec_at_recall_0.40  	all	0.3864
iprec_at_recall_0.50  	all	0.3094
iprec_at_recall_0.60  	all	0.2539
iprec_at_recall_0.70  	all	0.2077
iprec_at_recall_0.80  	all	0.1585
iprec_at_recall_0.90  	all	0.1045
iprec_at_recall_1.00  	all	0.0441
P_5                   	all	0.4600
P_10                  	all	0.4800
P_15                  	all	0.4587
P_20                  	all	0.4120
P_30                  	all	0.3473
P_100                 	all	0.1630
P_200                 	all	0.0926
P_500                 	all	0.0499
P_1000                	all	0.0333
</pre>
 <h4>MEDINFO.2.3.noadd</h4>
            <pre>
runid                 	all	MEDINFO23
num_q                 	all	50
num_ret               	all	47333
num_rel               	all	1878
num_rel_ret           	all	1609
map                   	all	0.2454
gm_map                	all	0.0881
Rprec                 	all	0.2721
bpref                 	all	0.3389
recip_rank            	all	0.5256
iprec_at_recall_0.00  	all	0.5943
iprec_at_recall_0.10  	all	0.5068
iprec_at_recall_0.20  	all	0.4524
iprec_at_recall_0.30  	all	0.3674
iprec_at_recall_0.40  	all	0.2866
iprec_at_recall_0.50  	all	0.2425
iprec_at_recall_0.60  	all	0.1946
iprec_at_recall_0.70  	all	0.1393
iprec_at_recall_0.80  	all	0.1000
iprec_at_recall_0.90  	all	0.0650
iprec_at_recall_1.00  	all	0.0189
P_5                   	all	0.4040
P_10                  	all	0.3980
P_15                  	all	0.3613
P_20                  	all	0.3260
P_30                  	all	0.2693
P_100                 	all	0.1366
P_200                 	all	0.0917
P_500                 	all	0.0517
P_1000                	all	0.0322
</pre>
 <h4>MEDINFO.3.3.noadd</h4>
            <pre>
runid                 	all	MEDINFO33
num_q                 	all	50
num_ret               	all	48866
num_rel               	all	1878
num_rel_ret           	all	1622
map                   	all	0.2584
gm_map                	all	0.0944
Rprec                 	all	0.2835
bpref                 	all	0.3434
recip_rank            	all	0.5567
iprec_at_recall_0.00  	all	0.6235
iprec_at_recall_0.10  	all	0.5245
iprec_at_recall_0.20  	all	0.4623
iprec_at_recall_0.30  	all	0.3619
iprec_at_recall_0.40  	all	0.2942
iprec_at_recall_0.50  	all	0.2518
iprec_at_recall_0.60  	all	0.2134
iprec_at_recall_0.70  	all	0.1608
iprec_at_recall_0.80  	all	0.1049
iprec_at_recall_0.90  	all	0.0651
iprec_at_recall_1.00  	all	0.0166
P_5                   	all	0.4280
P_10                  	all	0.4040
P_15                  	all	0.3627
P_20                  	all	0.3310
P_30                  	all	0.2780
P_100                 	all	0.1486
P_200                 	all	0.0991
P_500                 	all	0.0564
P_1000                	all	0.0324
</pre>
 <h4>MEDINFO.4.3.noadd</h4>
            <pre>
runid                 	all	MEDINFO43
num_q                 	all	50
num_ret               	all	47868
num_rel               	all	1878
num_rel_ret           	all	1618
map                   	all	0.2601
gm_map                	all	0.0960
Rprec                 	all	0.2867
bpref                 	all	0.3457
recip_rank            	all	0.5619
iprec_at_recall_0.00  	all	0.6275
iprec_at_recall_0.10  	all	0.5273
iprec_at_recall_0.20  	all	0.4611
iprec_at_recall_0.30  	all	0.3821
iprec_at_recall_0.40  	all	0.2988
iprec_at_recall_0.50  	all	0.2524
iprec_at_recall_0.60  	all	0.2144
iprec_at_recall_0.70  	all	0.1522
iprec_at_recall_0.80  	all	0.1098
iprec_at_recall_0.90  	all	0.0652
iprec_at_recall_1.00  	all	0.0190
P_5                   	all	0.4200
P_10                  	all	0.4060
P_15                  	all	0.3653
P_20                  	all	0.3350
P_30                  	all	0.2800
P_100                 	all	0.1466
P_200                 	all	0.1009
P_500                 	all	0.0558
P_1000                	all	0.0324
</pre>
 <h4>MEDINFO.5.3.noadd</h4>
            <pre>
runid                 	all	MEDINFO53
num_q                 	all	50
num_ret               	all	47333
num_rel               	all	1878
num_rel_ret           	all	1609
map                   	all	0.2426
gm_map                	all	0.0866
Rprec                 	all	0.2702
bpref                 	all	0.3368
recip_rank            	all	0.5198
iprec_at_recall_0.00  	all	0.5913
iprec_at_recall_0.10  	all	0.5085
iprec_at_recall_0.20  	all	0.4491
iprec_at_recall_0.30  	all	0.3628
iprec_at_recall_0.40  	all	0.2847
iprec_at_recall_0.50  	all	0.2388
iprec_at_recall_0.60  	all	0.1900
iprec_at_recall_0.70  	all	0.1380
iprec_at_recall_0.80  	all	0.0985
iprec_at_recall_0.90  	all	0.0627
iprec_at_recall_1.00  	all	0.0180
P_5                   	all	0.3960
P_10                  	all	0.4040
P_15                  	all	0.3573
P_20                  	all	0.3220
P_30                  	all	0.2687
P_100                 	all	0.1354
P_200                 	all	0.0913
P_500                 	all	0.0516
P_1000                	all	0.0322
</pre>
 <h4>MEDINFO.6.3.noadd</h4>
            <pre>
runid                 	all	MEDINFO63
num_q                 	all	50
num_ret               	all	47333
num_rel               	all	1878
num_rel_ret           	all	1605
map                   	all	0.2343
gm_map                	all	0.0867
Rprec                 	all	0.2561
bpref                 	all	0.3332
recip_rank            	all	0.5256
iprec_at_recall_0.00  	all	0.5986
iprec_at_recall_0.10  	all	0.4937
iprec_at_recall_0.20  	all	0.4401
iprec_at_recall_0.30  	all	0.3576
iprec_at_recall_0.40  	all	0.2723
iprec_at_recall_0.50  	all	0.2174
iprec_at_recall_0.60  	all	0.1714
iprec_at_recall_0.70  	all	0.1272
iprec_at_recall_0.80  	all	0.0899
iprec_at_recall_0.90  	all	0.0564
iprec_at_recall_1.00  	all	0.0182
P_5                   	all	0.3880
P_10                  	all	0.3600
P_15                  	all	0.3373
P_20                  	all	0.3060
P_30                  	all	0.2567
P_100                 	all	0.1416
P_200                 	all	0.0958
P_500                 	all	0.0504
P_1000                	all	0.0321
</pre>
 <h4>MEDINFO.7.3.noadd</h4>
            <pre>
runid                 	all	MEDINFO73
num_q                 	all	50
num_ret               	all	47333
num_rel               	all	1878
num_rel_ret           	all	1551
map                   	all	0.2174
gm_map                	all	0.0718
Rprec                 	all	0.2534
bpref                 	all	0.3250
recip_rank            	all	0.4872
iprec_at_recall_0.00  	all	0.5573
iprec_at_recall_0.10  	all	0.4596
iprec_at_recall_0.20  	all	0.3991
iprec_at_recall_0.30  	all	0.3345
iprec_at_recall_0.40  	all	0.2569
iprec_at_recall_0.50  	all	0.2070
iprec_at_recall_0.60  	all	0.1611
iprec_at_recall_0.70  	all	0.1205
iprec_at_recall_0.80  	all	0.0854
iprec_at_recall_0.90  	all	0.0506
iprec_at_recall_1.00  	all	0.0150
P_5                   	all	0.3560
P_10                  	all	0.3480
P_15                  	all	0.3267
P_20                  	all	0.3020
P_30                  	all	0.2480
P_100                 	all	0.1336
P_200                 	all	0.0899
P_500                 	all	0.0493
P_1000                	all	0.0310
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2013ehealth.1-50-test.graded.final.20062013.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2013ehealth.1-50-test.graded.final.20062013.txt runName</pre>
 <h4>MEDINFO.1.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.4189
ndcg_cut_10           	all	0.4377
ndcg_cut_15           	all	0.4371
ndcg_cut_20           	all	0.4299
ndcg_cut_30           	all	0.4243
ndcg_cut_100          	all	0.4767
ndcg_cut_200          	all	0.5034
ndcg_cut_500          	all	0.5461
ndcg_cut_1000         	all	0.5631
</pre>
 <h4>MEDINFO.2.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.3467
ndcg_cut_10           	all	0.3546
ndcg_cut_15           	all	0.3469
ndcg_cut_20           	all	0.3426
ndcg_cut_30           	all	0.3361
ndcg_cut_100          	all	0.3959
ndcg_cut_200          	all	0.4444
ndcg_cut_500          	all	0.4824
ndcg_cut_1000         	all	0.5050
</pre>
 <h4>MEDINFO.3.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.3703
ndcg_cut_10           	all	0.3639
ndcg_cut_15           	all	0.3553
ndcg_cut_20           	all	0.3528
ndcg_cut_30           	all	0.3479
ndcg_cut_100          	all	0.4127
ndcg_cut_200          	all	0.4578
ndcg_cut_500          	all	0.4966
ndcg_cut_1000         	all	0.5171
</pre>
 <h4>MEDINFO.4.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.3667
ndcg_cut_10           	all	0.3691
ndcg_cut_15           	all	0.3603
ndcg_cut_20           	all	0.3571
ndcg_cut_30           	all	0.3491
ndcg_cut_100          	all	0.4101
ndcg_cut_200          	all	0.4624
ndcg_cut_500          	all	0.4977
ndcg_cut_1000         	all	0.5157
</pre>
 <h4>MEDINFO.5.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.3407
ndcg_cut_10           	all	0.3561
ndcg_cut_15           	all	0.3425
ndcg_cut_20           	all	0.3386
ndcg_cut_30           	all	0.3329
ndcg_cut_100          	all	0.3919
ndcg_cut_200          	all	0.4395
ndcg_cut_500          	all	0.4795
ndcg_cut_1000         	all	0.5017
</pre>
 <h4>MEDINFO.6.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.3326
ndcg_cut_10           	all	0.3284
ndcg_cut_15           	all	0.3292
ndcg_cut_20           	all	0.3273
ndcg_cut_30           	all	0.3203
ndcg_cut_100          	all	0.3844
ndcg_cut_200          	all	0.4333
ndcg_cut_500          	all	0.4751
ndcg_cut_1000         	all	0.4967
</pre>
 <h4>MEDINFO.7.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.3061
ndcg_cut_10           	all	0.3075
ndcg_cut_15           	all	0.3092
ndcg_cut_20           	all	0.3104
ndcg_cut_30           	all	0.3041
ndcg_cut_100          	all	0.3707
ndcg_cut_200          	all	0.4129
ndcg_cut_500          	all	0.4532
ndcg_cut_1000         	all	0.4765
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar in then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>MEDINFO.1.3.noadd</h4>
<img src="./img/MEDINFO.1.3.noadd.p10.png"/>
 <h4>MEDINFO.2.3.noadd</h4>
<img src="./img/MEDINFO.2.3.noadd.p10.png"/>
 <h4>MEDINFO.3.3.noadd</h4>
<img src="./img/MEDINFO.3.3.noadd.p10.png"/>
 <h4>MEDINFO.4.3.noadd</h4>
<img src="./img/MEDINFO.4.3.noadd.p10.png"/>
 <h4>MEDINFO.5.3.noadd</h4>
<img src="./img/MEDINFO.5.3.noadd.p10.png"/>
 <h4>MEDINFO.6.3.noadd</h4>
<img src="./img/MEDINFO.6.3.noadd.p10.png"/>
 <h4>MEDINFO.7.3.noadd</h4>
<img src="./img/MEDINFO.7.3.noadd.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/bevankoopman/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
