<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2013 TASK 3 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2013 Task 3 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2013 TASK 3 Results - UTHealth_CCB</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2013 TASK 3 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the highest priority run that used the discharge summaries (run 2) and the highest priority run that did not used the discharge summaries (run 5). As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtestXX id); others did not retrieve results for some queries. To fairly evaluate the runs of every participant, submitted runs containing formatting errors have been corrected; an artificial document (fakeCLEF) has been added to the result rankings for queries where no result was retrieved. Your runs as have been evaluated are contained in the folder <i>runs</i>.</br>Please refer to the <a href="https://sites.google.com/site/shareclefehealth/">ShARe/CLEF 2013 eHealth Evaluation Lab website</a> for additional details on the task.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2013ehealth.1-50-test.bin.final.20062013.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2013ehealth.1-50-test.bin.final.20062013.txt runName</pre>
 <h4>UTHealth_CCB.1.3.noadd</h4>
            <pre>
runid                 	all	UTHealth_CCB.1.3.noadd
num_q                 	all	50
num_ret               	all	2451
num_rel               	all	1878
num_rel_ret           	all	458
map                   	all	0.1482
gm_map                	all	0.0312
Rprec                 	all	0.2148
bpref                 	all	0.2056
recip_rank            	all	0.5382
iprec_at_recall_0.00  	all	0.6048
iprec_at_recall_0.10  	all	0.4529
iprec_at_recall_0.20  	all	0.3579
iprec_at_recall_0.30  	all	0.2498
iprec_at_recall_0.40  	all	0.1462
iprec_at_recall_0.50  	all	0.0623
iprec_at_recall_0.60  	all	0.0321
iprec_at_recall_0.70  	all	0.0169
iprec_at_recall_0.80  	all	0.0030
iprec_at_recall_0.90  	all	0.0000
iprec_at_recall_1.00  	all	0.0000
P_5                   	all	0.3920
P_10                  	all	0.3740
P_15                  	all	0.3200
P_20                  	all	0.2840
P_30                  	all	0.2327
P_100                 	all	0.0916
P_200                 	all	0.0458
P_500                 	all	0.0183
P_1000                	all	0.0092
</pre>
 <h4>UTHealth_CCB.5.3.noadd</h4>
            <pre>
runid                 	all	UTHealth_CCB.5.3.noadd
num_q                 	all	50
num_ret               	all	2451
num_rel               	all	1878
num_rel_ret           	all	296
map                   	all	0.0953
gm_map                	all	0.0058
Rprec                 	all	0.1569
bpref                 	all	0.1458
recip_rank            	all	0.4680
iprec_at_recall_0.00  	all	0.5006
iprec_at_recall_0.10  	all	0.3424
iprec_at_recall_0.20  	all	0.1818
iprec_at_recall_0.30  	all	0.1164
iprec_at_recall_0.40  	all	0.0668
iprec_at_recall_0.50  	all	0.0271
iprec_at_recall_0.60  	all	0.0183
iprec_at_recall_0.70  	all	0.0143
iprec_at_recall_0.80  	all	0.0043
iprec_at_recall_0.90  	all	0.0000
iprec_at_recall_1.00  	all	0.0000
P_5                   	all	0.2600
P_10                  	all	0.2540
P_15                  	all	0.2160
P_20                  	all	0.1840
P_30                  	all	0.1467
P_100                 	all	0.0592
P_200                 	all	0.0296
P_500                 	all	0.0118
P_1000                	all	0.0059
</pre>
 <h4>UTHealth_CCB.6.3.noadd</h4>
            <pre>
runid                 	all	UTHealth_CCB.6.3.noadd
num_q                 	all	50
num_ret               	all	2500
num_rel               	all	1878
num_rel_ret           	all	337
map                   	all	0.1124
gm_map                	all	0.0084
Rprec                 	all	0.1691
bpref                 	all	0.1875
recip_rank            	all	0.3859
iprec_at_recall_0.00  	all	0.4459
iprec_at_recall_0.10  	all	0.3158
iprec_at_recall_0.20  	all	0.2779
iprec_at_recall_0.30  	all	0.2042
iprec_at_recall_0.40  	all	0.1079
iprec_at_recall_0.50  	all	0.0520
iprec_at_recall_0.60  	all	0.0323
iprec_at_recall_0.70  	all	0.0253
iprec_at_recall_0.80  	all	0.0198
iprec_at_recall_0.90  	all	0.0000
iprec_at_recall_1.00  	all	0.0000
P_5                   	all	0.2760
P_10                  	all	0.2560
P_15                  	all	0.2240
P_20                  	all	0.1920
P_30                  	all	0.1680
P_100                 	all	0.0674
P_200                 	all	0.0337
P_500                 	all	0.0135
P_1000                	all	0.0067
</pre>
 <h4>UTHealth_CCB.7.3.noadd</h4>
            <pre>
runid                 	all	UTHealth_CCB.7.3.noadd
num_q                 	all	50
num_ret               	all	2500
num_rel               	all	1878
num_rel_ret           	all	204
map                   	all	0.0546
gm_map                	all	0.0027
Rprec                 	all	0.0984
bpref                 	all	0.1140
recip_rank            	all	0.2848
iprec_at_recall_0.00  	all	0.3239
iprec_at_recall_0.10  	all	0.1987
iprec_at_recall_0.20  	all	0.1006
iprec_at_recall_0.30  	all	0.0677
iprec_at_recall_0.40  	all	0.0342
iprec_at_recall_0.50  	all	0.0223
iprec_at_recall_0.60  	all	0.0121
iprec_at_recall_0.70  	all	0.0000
iprec_at_recall_0.80  	all	0.0000
iprec_at_recall_0.90  	all	0.0000
iprec_at_recall_1.00  	all	0.0000
P_5                   	all	0.1680
P_10                  	all	0.1460
P_15                  	all	0.1400
P_20                  	all	0.1190
P_30                  	all	0.0993
P_100                 	all	0.0408
P_200                 	all	0.0204
P_500                 	all	0.0082
P_1000                	all	0.0041
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2013ehealth.1-50-test.graded.final.20062013.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2013ehealth.1-50-test.graded.final.20062013.txt runName</pre>
 <h4>UTHealth_CCB.1.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.3444
ndcg_cut_10           	all	0.3406
ndcg_cut_15           	all	0.3215
ndcg_cut_20           	all	0.3128
ndcg_cut_30           	all	0.3008
ndcg_cut_100          	all	0.3040
ndcg_cut_200          	all	0.3001
ndcg_cut_500          	all	0.2969
ndcg_cut_1000         	all	0.2964
</pre>
 <h4>UTHealth_CCB.5.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.2681
ndcg_cut_10           	all	0.2587
ndcg_cut_15           	all	0.2415
ndcg_cut_20           	all	0.2279
ndcg_cut_30           	all	0.2174
ndcg_cut_100          	all	0.2197
ndcg_cut_200          	all	0.2197
ndcg_cut_500          	all	0.2197
ndcg_cut_1000         	all	0.2197
</pre>
 <h4>UTHealth_CCB.6.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.2384
ndcg_cut_10           	all	0.2337
ndcg_cut_15           	all	0.2269
ndcg_cut_20           	all	0.2161
ndcg_cut_30           	all	0.2187
ndcg_cut_100          	all	0.2318
ndcg_cut_200          	all	0.2318
ndcg_cut_500          	all	0.2318
ndcg_cut_1000         	all	0.2318
</pre>
 <h4>UTHealth_CCB.7.3.noadd</h4>
            <pre>
ndcg_cut_5            	all	0.1442
ndcg_cut_10           	all	0.1368
ndcg_cut_15           	all	0.1428
ndcg_cut_20           	all	0.1358
ndcg_cut_30           	all	0.1345
ndcg_cut_100          	all	0.1413
ndcg_cut_200          	all	0.1413
ndcg_cut_500          	all	0.1413
ndcg_cut_1000         	all	0.1413
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar in then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>UTHealth_CCB.1.3.noadd</h4>
<img src="./img/UTHealth_CCB.1.3.noadd.p10.png"/>
 <h4>UTHealth_CCB.5.3.noadd</h4>
<img src="./img/UTHealth_CCB.5.3.noadd.p10.png"/>
 <h4>UTHealth_CCB.6.3.noadd</h4>
<img src="./img/UTHealth_CCB.6.3.noadd.p10.png"/>
 <h4>UTHealth_CCB.7.3.noadd</h4>
<img src="./img/UTHealth_CCB.7.3.noadd.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/bevankoopman/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
